{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "E6_SongWriter.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1qelIp08y9P",
        "outputId": "17f7e6cd-47e1-48bd-ba29-ab9b4e879857"
      },
      "source": [
        "!wget https://aiffelstaticprd.blob.core.windows.net/media/documents/song_lyrics.zip\n",
        "!unzip song_lyrics.zip -d ."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-01 02:55:21--  https://aiffelstaticprd.blob.core.windows.net/media/documents/song_lyrics.zip\n",
            "Resolving aiffelstaticprd.blob.core.windows.net (aiffelstaticprd.blob.core.windows.net)... 52.239.148.4\n",
            "Connecting to aiffelstaticprd.blob.core.windows.net (aiffelstaticprd.blob.core.windows.net)|52.239.148.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2101791 (2.0M) [application/zip]\n",
            "Saving to: ‘song_lyrics.zip’\n",
            "\n",
            "song_lyrics.zip     100%[===================>]   2.00M  1.37MB/s    in 1.5s    \n",
            "\n",
            "2021-02-01 02:55:23 (1.37 MB/s) - ‘song_lyrics.zip’ saved [2101791/2101791]\n",
            "\n",
            "Archive:  song_lyrics.zip\n",
            "  inflating: ./Kanye_West.txt        \n",
            "  inflating: ./Lil_Wayne.txt         \n",
            "  inflating: ./adele.txt             \n",
            "  inflating: ./al-green.txt          \n",
            "  inflating: ./alicia-keys.txt       \n",
            "  inflating: ./amy-winehouse.txt     \n",
            "  inflating: ./beatles.txt           \n",
            "  inflating: ./bieber.txt            \n",
            "  inflating: ./bjork.txt             \n",
            "  inflating: ./blink-182.txt         \n",
            "  inflating: ./bob-dylan.txt         \n",
            "  inflating: ./bob-marley.txt        \n",
            "  inflating: ./britney-spears.txt    \n",
            "  inflating: ./bruce-springsteen.txt  \n",
            "  inflating: ./bruno-mars.txt        \n",
            "  inflating: ./cake.txt              \n",
            "  inflating: ./dickinson.txt         \n",
            "  inflating: ./disney.txt            \n",
            "  inflating: ./dj-khaled.txt         \n",
            "  inflating: ./dolly-parton.txt      \n",
            "  inflating: ./dr-seuss.txt          \n",
            "  inflating: ./drake.txt             \n",
            "  inflating: ./eminem.txt            \n",
            "  inflating: ./janisjoplin.txt       \n",
            "  inflating: ./jimi-hendrix.txt      \n",
            "  inflating: ./johnny-cash.txt       \n",
            "  inflating: ./joni-mitchell.txt     \n",
            "  inflating: ./kanye-west.txt        \n",
            "  inflating: ./kanye.txt             \n",
            "  inflating: ./lady-gaga.txt         \n",
            "  inflating: ./leonard-cohen.txt     \n",
            "  inflating: ./lil-wayne.txt         \n",
            "  inflating: ./lin-manuel-miranda.txt  \n",
            "  inflating: ./lorde.txt             \n",
            "  inflating: ./ludacris.txt          \n",
            "  inflating: ./michael-jackson.txt   \n",
            "  inflating: ./missy-elliott.txt     \n",
            "  inflating: ./nickelback.txt        \n",
            "  inflating: ./nicki-minaj.txt       \n",
            "  inflating: ./nirvana.txt           \n",
            "  inflating: ./notorious-big.txt     \n",
            "  inflating: ./notorious_big.txt     \n",
            "  inflating: ./nursery_rhymes.txt    \n",
            "  inflating: ./patti-smith.txt       \n",
            "  inflating: ./paul-simon.txt        \n",
            "  inflating: ./prince.txt            \n",
            "  inflating: ./r-kelly.txt           \n",
            "  inflating: ./radiohead.txt         \n",
            "  inflating: ./rihanna.txt           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfCwXKSJ9JH7",
        "outputId": "9746b789-8b77-4941-d5b3-dfe75bd3872d"
      },
      "source": [
        "import re                  # 정규표현식을 위한 Regex 지원 모듈 (문장 데이터를 정돈하기 위해)\n",
        "import numpy as np         # 변환된 문장 데이터(행렬)을 편하게 처리하기 위해\n",
        "import tensorflow as tf    # 대망의 텐서플로우!\n",
        "import os\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "txt_file_path = '/content/*.txt'\n",
        "\n",
        "txt_list = glob.glob(txt_file_path)\n",
        "\n",
        "raw_corpus = []\n",
        "\n",
        "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines()\n",
        "        raw_corpus.extend(raw)\n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:3])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "데이터 크기: 187088\n",
            "Examples:\n",
            " ['What do you mean?', 'Oh, oh, oh', 'When you sometimes say yes']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kunMnwA59nAI"
      },
      "source": [
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다.\n",
        "    while True:\n",
        "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다.\n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다.\n",
        "\n",
        "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다.\n",
        "        test_tensor = tf.concat([test_tensor,\n",
        "                                 tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "\n",
        "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다.\n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다.\n",
        "\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
        "\n",
        "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
        "\n",
        "    sentence = sentence.strip()\n",
        "    \n",
        "    # 토큰의 개수가 15보다 큰 문장 제외\n",
        "    if len(sentence.split()) > 15:\n",
        "        splitted_str = sentence.split()\n",
        "        temp=''\n",
        "        for idx, s in enumerate(splitted_str):\n",
        "            if idx > 15:\n",
        "                break\n",
        "            temp.join(s)\n",
        "        sentence = temp\n",
        "\n",
        "        # sentence = str(sentence.split()[:15])\n",
        "\n",
        "\n",
        "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
        "\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def tokenize(corpus):\n",
        "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=12000,  # 전체 단어의 개수\n",
        "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
        "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
        "    )\n",
        "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
        "\n",
        "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
        "\n",
        "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
        "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fRbiz-vgGI2"
      },
      "source": [
        "# val-loss를 목표치로 맞추기위해 이 부분만 따로 실행하도록 배치\n",
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super(TextGenerator, self).__init__()\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "\n",
        "        return out\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmQW6ZROQf4W",
        "outputId": "13fd1ea3-25cd-46f8-8a94-8094df96b40e"
      },
      "source": [
        "\n",
        "corpus = []\n",
        "\n",
        "# 이 프로젝트에서는 길이가 0인 즉 빈줄만 제\n",
        "for sentence in raw_corpus:\n",
        "    if len(sentence) == 0: continue\n",
        "    corpus.append(preprocess_sentence(sentence))\n",
        "\n",
        "print(\"데이터 크기:\", len(corpus))\n",
        "print(\"Examples:\\n\", corpus[:3])\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)\n",
        "\n",
        "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
        "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
        "\n",
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=20)\n",
        "\n",
        "# 124960보다 크다면 위 Step 3.의 데이터 정제 과정을 다시한번 검토\n",
        "print(\"Source Train:\", enc_train.shape)\n",
        "print(\"Target Train:\", dec_train.shape)\n",
        "\n",
        "BUFFER_SIZE = len(src_input)\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
        "\n",
        "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "\n",
        "embedding_size = 256\n",
        "hidden_size = 1024\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
        "\n",
        "for src_sample, tgt_sample in dataset.take(1): break\n",
        "model(src_sample)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none'\n",
        ")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "데이터 크기: 175986\n",
            "Examples:\n",
            " ['<start> what do you mean ? <end>', '<start> oh , oh , oh <end>', '<start> when you sometimes say yes <end>']\n",
            "[[  2  39  48 ...   0   0   0]\n",
            " [  2  45   5 ...   0   0   0]\n",
            " [  2  47   7 ...   0   0   0]\n",
            " ...\n",
            " [  2  41 894 ...   0   0   0]\n",
            " [  2  41  68 ...   0   0   0]\n",
            " [  2   8  83 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f18716bc7b8>\n",
            "Source Train: (140788, 16)\n",
            "Target Train: (140788, 16)\n",
            "Model: \"text_generator_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      multiple                  3072256   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                multiple                  5246976   \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                multiple                  8392704   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  12301025  \n",
            "=================================================================\n",
            "Total params: 29,012,961\n",
            "Trainable params: 29,012,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvSLehyAQr5n",
        "outputId": "09bced02-386b-4831-c19e-43d6b189e39d"
      },
      "source": [
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "model.fit(enc_train, dec_train, epochs=10, validation_data=(enc_val, dec_val))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "4400/4400 [==============================] - 248s 56ms/step - loss: 2.8651 - val_loss: 2.3928\n",
            "Epoch 2/10\n",
            "4400/4400 [==============================] - 247s 56ms/step - loss: 2.2819 - val_loss: 2.2193\n",
            "Epoch 3/10\n",
            "4400/4400 [==============================] - 246s 56ms/step - loss: 2.0229 - val_loss: 2.1224\n",
            "Epoch 4/10\n",
            "4400/4400 [==============================] - 240s 54ms/step - loss: 1.7875 - val_loss: 2.0625\n",
            "Epoch 5/10\n",
            "4400/4400 [==============================] - 236s 54ms/step - loss: 1.5877 - val_loss: 2.0235\n",
            "Epoch 6/10\n",
            "4400/4400 [==============================] - 236s 54ms/step - loss: 1.4161 - val_loss: 2.0112\n",
            "Epoch 7/10\n",
            "4400/4400 [==============================] - 235s 53ms/step - loss: 1.2814 - val_loss: 2.0113\n",
            "Epoch 8/10\n",
            "4400/4400 [==============================] - 236s 54ms/step - loss: 1.1648 - val_loss: 2.0204\n",
            "Epoch 9/10\n",
            "4400/4400 [==============================] - 235s 53ms/step - loss: 1.0752 - val_loss: 2.0461\n",
            "Epoch 10/10\n",
            "4400/4400 [==============================] - 235s 53ms/step - loss: 1.0048 - val_loss: 2.0771\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f18717a99b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZxB1SDffQ6sg",
        "outputId": "5146687f-4b97-4bcd-c57d-3d6bc0781d34"
      },
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> i love\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> i love you <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XBQWOI0fVSP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxRtMmHvz8ld"
      },
      "source": [
        "### 별다른 모델 수정 없이 val-loss가 2.0771로 10 epoch만에 낮아졌다.\n",
        "### 물론 현재 training shape를 보면 프로젝트 지시사항에 나오는 데이터보다 많이 존재한다.\n",
        "### 프로젝트 지시사항에서는 전처리후 나온 데이터셋 크기가 124960 인데 여기서는 140788이 나온다."
      ]
    }
  ]
}