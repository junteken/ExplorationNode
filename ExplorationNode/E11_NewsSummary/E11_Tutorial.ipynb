{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "above-question",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/beanzsoft/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 100000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import urllib.request\n",
    "\n",
    "# data = pd.read_csv(os.getenv(\"HOME\")+\"/aiffel/news_summarization/data/Reviews.csv\", nrows = 100000)\n",
    "data = pd.read_csv(os.getcwd()+\"/data/Reviews.csv\", nrows = 100000)\n",
    "print('전체 샘플수 :',(len(data)))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "broken-steps",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58956</th>\n",
       "      <td>The food is great, but the bag that I got from...</td>\n",
       "      <td>Great food, but don't buy it here.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36006</th>\n",
       "      <td>I have tried various Flavia's and K-cup for ye...</td>\n",
       "      <td>One of the better coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85730</th>\n",
       "      <td>I got this as a gift from my grandparents when...</td>\n",
       "      <td>Yummy!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26173</th>\n",
       "      <td>But you will enjoy ever step. I gained 5 lbs w...</td>\n",
       "      <td>These chips will make you fat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21629</th>\n",
       "      <td>I've been using agave for a few years now and ...</td>\n",
       "      <td>edges out the others taste wise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70514</th>\n",
       "      <td>Nana's Chocolate and chocolate Chip cookies ar...</td>\n",
       "      <td>Tasty Cookies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82519</th>\n",
       "      <td>Add to water, stir and voila! Delicious slight...</td>\n",
       "      <td>Love love love visinada!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71218</th>\n",
       "      <td>I was not impressed about this product (Made i...</td>\n",
       "      <td>Sweet Leather for your dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2510</th>\n",
       "      <td>I picked this up at my local grocery store in ...</td>\n",
       "      <td>I Want More!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54284</th>\n",
       "      <td>*****&lt;br /&gt;Numi Tea's Monkey King Jasmine Gree...</td>\n",
       "      <td>Green Tea with Jasmine, Nothing Else!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91027</th>\n",
       "      <td>We love tuna and eat it on a regular basis and...</td>\n",
       "      <td>I Love it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67845</th>\n",
       "      <td>Chips are not as tangy as I hoped or expected....</td>\n",
       "      <td>Great price, but not as tangy as I expected.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25790</th>\n",
       "      <td>A very tasty, classy salt adds a bit of spice ...</td>\n",
       "      <td>a very fine salt!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85407</th>\n",
       "      <td>nothing about this jerky stands out.  might as...</td>\n",
       "      <td>mediocre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2274</th>\n",
       "      <td>I am fairly certain this is the same tea sold ...</td>\n",
       "      <td>Delicious tea!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  \\\n",
       "58956  The food is great, but the bag that I got from...   \n",
       "36006  I have tried various Flavia's and K-cup for ye...   \n",
       "85730  I got this as a gift from my grandparents when...   \n",
       "26173  But you will enjoy ever step. I gained 5 lbs w...   \n",
       "21629  I've been using agave for a few years now and ...   \n",
       "70514  Nana's Chocolate and chocolate Chip cookies ar...   \n",
       "82519  Add to water, stir and voila! Delicious slight...   \n",
       "71218  I was not impressed about this product (Made i...   \n",
       "2510   I picked this up at my local grocery store in ...   \n",
       "54284  *****<br />Numi Tea's Monkey King Jasmine Gree...   \n",
       "91027  We love tuna and eat it on a regular basis and...   \n",
       "67845  Chips are not as tangy as I hoped or expected....   \n",
       "25790  A very tasty, classy salt adds a bit of spice ...   \n",
       "85407  nothing about this jerky stands out.  might as...   \n",
       "2274   I am fairly certain this is the same tea sold ...   \n",
       "\n",
       "                                            Summary  \n",
       "58956            Great food, but don't buy it here.  \n",
       "36006                      One of the better coffee  \n",
       "85730                                        Yummy!  \n",
       "26173                 These chips will make you fat  \n",
       "21629               edges out the others taste wise  \n",
       "70514                                 Tasty Cookies  \n",
       "82519                      Love love love visinada!  \n",
       "71218                    Sweet Leather for your dog  \n",
       "2510                                   I Want More!  \n",
       "54284         Green Tea with Jasmine, Nothing Else!  \n",
       "91027                                  I Love it...  \n",
       "67845  Great price, but not as tangy as I expected.  \n",
       "25790                             a very fine salt!  \n",
       "85407                                      mediocre  \n",
       "2274                                 Delicious tea!  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['Text','Summary']]\n",
    "data.head()\n",
    "\n",
    "#랜덤한 15개 샘플 출력\n",
    "data.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "confidential-lotus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 열에서 중복을 배제한 유일한 샘플의 수 : 88426\n",
      "Summary 열에서 중복을 배제한 유일한 샘플의 수 : 72348\n"
     ]
    }
   ],
   "source": [
    "print('Text 열에서 중복을 배제한 유일한 샘플의 수 :', data['Text'].nunique())\n",
    "print('Summary 열에서 중복을 배제한 유일한 샘플의 수 :', data['Summary'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "armed-titanium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88425\n"
     ]
    }
   ],
   "source": [
    "# 중복을 제외한다면 Text에는 88,426개, \n",
    "# Summary에는 72,348개의 의 유니크한 데이터가 존재해요. \n",
    "# 사실 이 데이터의 Summary는 'Smelly'나 'Good Product'와 같이 아주 간단한\n",
    "# 요약들도 많아서 Text가 달라도 Summary는 동일할 수 있어요. \n",
    "# 하지만 Text 자체가 중복이 된 경우는 중복 샘플이므로 제거해야겠죠.\n",
    "# 데이터프레임의 drop_duplicates()를 사용하면, 손쉽게 중복 샘플을 제거할 수 있어요.\n",
    "data.drop_duplicates(subset = ['Text'], inplace = True)\n",
    "print('전체 샘플수 :',(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "desperate-tampa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text       0\n",
      "Summary    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "proper-traffic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88425\n"
     ]
    }
   ],
   "source": [
    "data.dropna(axis = 0, inplace = True)\n",
    "print('전체 샘플수 :',(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-latest",
   "metadata": {},
   "source": [
    " 'it'll'은 'it will'과 같고, 'mustn't과 'must not'은 사실 같은 표현이죠. 이런 경우 기계가 굳이 이들을 마치 다른 단어로 간주하게 해서 연산량을 늘리는 것보다는 기계 학습 전에 미리 같은 표현으로 통일시켜주는 것이 기계의 연산량을 줄일 수 있는 방법이에요.\n",
    "\n",
    "이러한 방법론을 텍스트 처리에서는 텍스트 정규화(text normalization)\n",
    "https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beginning-motel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규화 사전의 수:  120\n"
     ]
    }
   ],
   "source": [
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "print(\"정규화 사전의 수: \",len(contractions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "friendly-lounge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print('불용어 개수 :', len(stopwords.words('english') ))\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-bargain",
   "metadata": {},
   "source": [
    "NLTK에서 미리 정의하여 제공하고 있는 불용어는 총 179를 볼 수 있죠. 이를 사용하여 불용어를 제거할거에요. 이 작업 외에도 모든 영어문자는 소문자로 만들고, 섞여있는 html 태그를 제거하고, 정규 표현식을 통해 각종 특수문자를 제거해서 정말 필요한 내용만 잘 학습할 수 있도록 처리할거에요.\n",
    "\n",
    "함수의 하단을 보면, NLTK를 이용해 불용어를 제거하는 파트가 있는데, 이는 Text 전처리 시에서만 호출하고 이미 상대적으로 문장 길이가 짧은 Summary 전처리할 때는 호출하지 않을 예정이에요. Abstractive한 문장 요약 결과문이 자연스러운 문장이 되려면 이 불용어들이 Summary에는 남아 있는게 더 좋을 것 같습니다. 이 처리를 위해서 함수의 인자로 remove_stopwords를 추가하고, if문을 추가했어요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "historical-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 전처리 함수\n",
    "def preprocess_sentence(sentence, remove_stopwords=True):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for\n",
    "    sentence = re.sub('\"','', sentence) # 쌍따옴표 \" 제거\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n",
    "    sentence = re.sub(r\"'s\\b\",\"\",sentence) # 소유격 제거. Ex) roland's -> roland\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n",
    "    \n",
    "    # 불용어 제거 (Text)\n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)\n",
    "    # 불용어 미제거 (Summary)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "paperback-conservation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "everything bought great infact ordered twice third ordered wasfor mother father\n",
      "great way to start the day\n"
     ]
    }
   ],
   "source": [
    "temp_text = 'Everything I bought was great, infact I ordered twice and the third ordered was<br />for my mother and father.'\n",
    "temp_summary = 'Great way to start (or finish) the day!!!'\n",
    "\n",
    "print(preprocess_sentence(temp_text))\n",
    "print(preprocess_sentence(temp_summary, False))  # 불용어를 제거하지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-trustee",
   "metadata": {},
   "source": [
    "결과를 보면 기본적으로 모든 알파벳이 소문자로 변환되고,\n",
    "과 같은 html 태그가 제거되었죠. 또한 (or finish)와 같은 괄호로 묶였던 단어 시퀀스가 제거된 것도 확인할 수 있어요. 그리고 특수문자가 제거되면서 영어만 남았어요.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "conventional-designation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better',\n",
       " 'product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo',\n",
       " 'confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story lewis lion witch wardrobe treat seduces edmund selling brother sisters witch',\n",
       " 'looking secret ingredient robitussin believe found got addition root beer extract ordered made cherry soda flavor medicinal',\n",
       " 'great taffy great price wide assortment yummy taffy delivery quick taffy lover deal']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text = []\n",
    "\n",
    "# 전체 Text 데이터에 대한 전처리 : 10분 이상 시간이 걸릴 수 있습니다. \n",
    "for s in data['Text']:\n",
    "    clean_text.append(preprocess_sentence(s))\n",
    "\n",
    "# 전처리 후 출력\n",
    "clean_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "welsh-chambers",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beanzsoft/.conda/envs/AIFFEL/lib/python3.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.amazon.com/gp/product/b007i7yygy/ref=cm_cr_rev_prod_title\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['good quality dog food',\n",
       " 'not as advertised',\n",
       " 'delight says it all',\n",
       " 'cough medicine',\n",
       " 'great taffy']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_summary = []\n",
    "\n",
    "# 전체 Summary 데이터에 대한 전처리 : 5분 이상 시간이 걸릴 수 있습니다. \n",
    "for s in data['Summary']:\n",
    "    clean_summary.append(preprocess_sentence(s, False))\n",
    "\n",
    "clean_summary[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "molecular-detective",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Text'] = clean_text\n",
    "data['Summary'] = clean_summary\n",
    "\n",
    "# 빈 값을 Null 값으로 변환\n",
    "data.replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "vietnamese-temple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text        0\n",
       "Summary    70\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "crude-gabriel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88355\n"
     ]
    }
   ],
   "source": [
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :',(len(data)))#데이터 전처리 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-colonial",
   "metadata": {},
   "source": [
    "학습을 진행하기 위해서는 학습에 사용할 데이터의 크기를 결정하고, 문장의 시작과 끝을 표시 해줘야해요.\n",
    "\n",
    "샘플의 최대 길이 정하기\n",
    "필요없는 단어를 모두 솎아낸 데이터를 가지게 되었으니, 이제 훈련에 사용할 샘플의 최대 길이를 정해줄 차례에요.\n",
    "\n",
    "Text와 Summary의 최소, 최대, 평균 길이를 구하고 또한 길이 분포를 시각화해서 볼게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "economic-coalition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트의 최소 길이 : 2\n",
      "텍스트의 최대 길이 : 1235\n",
      "텍스트의 평균 길이 : 38.792428272310566\n",
      "요약의 최소 길이 : 1\n",
      "요약의 최대 길이 : 28\n",
      "요약의 평균 길이 : 4.010729443721352\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 길이 분포 출력\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_len = [len(s.split()) for s in data['Text']]\n",
    "summary_len = [len(s.split()) for s in data['Summary']]\n",
    "\n",
    "print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n",
    "print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n",
    "print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n",
    "print('요약의 최소 길이 : {}'.format(np.min(summary_len)))\n",
    "print('요약의 최대 길이 : {}'.format(np.max(summary_len)))\n",
    "print('요약의 평균 길이 : {}'.format(np.mean(summary_len)))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(summary_len)\n",
    "plt.title('Summary')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(text_len)\n",
    "plt.title('Text')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Summary')\n",
    "plt.hist(summary_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "plt.title('Text')\n",
    "plt.hist(text_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-differential",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text의 경우 최소 길이가 2, 최대 길이가 1,235으로 그 차이가 굉장히 크죠.\n",
    "하지만 평균 길이는 38로 시각화 된 그래프로 봤을 때는 대체적으로는 100 내외의 길이를 가진다는 것을 확인할 수 있어요.\n",
    "Summary의 경우 최소 길이가 1, 최대 길이가 28, 그리고 평균 길이가 4로 Text에 비해 상대적으로 길이가 매우 짧아요.\n",
    "그래프로 봤을 때에도 대체적으로 10이하의 길이를 가지고 있네요.\n",
    "이로부터 Text의 최대 길이와 Summary의 적절한 최대 길이를 임의로 정해볼게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "engaging-longer",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_max_len = 50\n",
    "summary_max_len = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "centered-metabolism",
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "  cnt = 0\n",
    "  for s in nested_list:\n",
    "    if(len(s.split()) <= max_len):\n",
    "        cnt = cnt + 1\n",
    "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "great-cause",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 50 이하인 샘플의 비율: 0.7745119121724859\n",
      "전체 샘플 중 길이가 8 이하인 샘플의 비율: 0.9424593967517402\n"
     ]
    }
   ],
   "source": [
    "below_threshold_len(text_max_len, data['Text'])\n",
    "below_threshold_len(summary_max_len,  data['Summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-advancement",
   "metadata": {},
   "source": [
    "각각 50과 8로 패딩을 하게되면 해당 길이보다 긴 샘플들은 내용이 잘리게 되는데, Text 열의 경우에는 약 23%의 샘플들이 내용이 망가지게 된다고 하네요.\n",
    "\n",
    "우리는 정해진 길이에 맞춰 자르는 것이 아니라, 정해진 길이보다 길면 제외하는 방법으로 데이터를 정제할게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ahead-nicholas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 65818\n"
     ]
    }
   ],
   "source": [
    "data = data[data['Text'].apply(lambda x: len(x.split()) <= text_max_len)]\n",
    "data = data[data['Summary'].apply(lambda x: len(x.split()) <= summary_max_len)]\n",
    "print('전체 샘플수 :',(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-colleague",
   "metadata": {},
   "source": [
    "시작 토큰과 종료 토큰 추가하기\n",
    "앞서 시작 토큰과 종료 토큰에 대해서 언급했던 것을 기억하시나요? 디코더는 시작 토큰을 입력받아 문장을 생성하기 시작하고, 종료 토큰을 예측한 순간에 문장 생성을 멈추는 거였죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "vulnerable-maine",
   "metadata": {},
   "outputs": [],
   "source": [
    "#요약 데이터에는 시작 토큰과 종료 토큰을 추가한다.\n",
    "data['decoder_input'] = data['Summary'].apply(lambda x : 'sostoken '+ x)\n",
    "data['decoder_target'] = data['Summary'].apply(lambda x : x + ' eostoken')\n",
    "data.head()\n",
    "\n",
    "encoder_input = np.array(data['Text']) # 인코더의 입력\n",
    "decoder_input = np.array(data['decoder_input']) # 디코더의 입력\n",
    "decoder_target = np.array(data['decoder_target']) # 디코더의 레이블"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-meditation",
   "metadata": {},
   "source": [
    "훈련 데이터와 테스트 데이터를 분리하는 방법은 분리 패키지를 사용하는 방법, 또는 직접 코딩을 통해서 분리하는 방법 등 여러가지 방법이 있을텐데 여기서는 직접 해볼게요. 우선, encoder_input과 크기와 형태가 같은 순서가 섞인 정수 시퀀스를 만들어줄게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "above-tobacco",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33994 57192 22916 ... 12395 55171 17615]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "widespread-integrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "defined-strength",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터의 수 : 13163\n"
     ]
    }
   ],
   "source": [
    "n_of_val = int(len(encoder_input)*0.2)\n",
    "print('테스트 데이터의 수 :',n_of_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "substantial-syndicate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 개수 : 52655\n",
      "훈련 레이블의 개수 : 52655\n",
      "테스트 데이터의 개수 : 13163\n",
      "테스트 레이블의 개수 : 13163\n"
     ]
    }
   ],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :',len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :',len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :',len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-version",
   "metadata": {},
   "source": [
    "단어 집합(vocaburary) 만들기 및 정수 인코딩\n",
    "이제 기계가 텍스트를 숫자로 처리할 수 있도록 훈련 데이터와 테스트 데이터의 단어들을 모두 정수로 바꾸어 주어야 해요. 이를 위해서는 각 단어에 고유한 정수를 맵핑하는 작업이 필요해요. 이 과정을 단어 집합(vocaburary)을 만든다고 표현해요. 훈련 데이터에 대해서 단어 집합을 만들어볼게요. 우선, 원문에 해당되는 encoder_input_train에 대해서 단어 집합을 만들게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "detailed-premiere",
   "metadata": {},
   "outputs": [],
   "source": [
    "ㅊ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-kingdom",
   "metadata": {},
   "source": [
    "이제 단어 집합이 생성되는 동시에 각 단어에 고유한 정수가 부여되었어요. 현재 생성된 단어 집합은 src_tokenizer.word_index에 저장되어있어요. 그런데 우리는 이렇게 만든 단어 집합에 있는 모든 단어를 사용하는 것이 아니라, 빈도수가 낮은 단어들은 훈련 데이터에서 제외하고 진행하려고 해요.\n",
    "\n",
    "등장 빈도수가 7회 미만인 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인해볼게요.\n",
    "\n",
    "src_tokenizer.word_counts.items()에는 단어와 각 단어의 등장 빈도수가 저장되어져 있는데, 이를 통해서 통계적인 정보를 얻을 수 있어요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "hundred-banks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 31977\n",
      "등장 빈도가 6번 이하인 희귀 단어의 수: 23713\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 8264\n",
      "단어 집합에서 희귀 단어의 비율: 74.15642493041874\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 3.370923609296233\n"
     ]
    }
   ],
   "source": [
    "threshold = 7\n",
    "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in src_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-nomination",
   "metadata": {},
   "source": [
    "encoder_input_train에는 총 32,017개의 단어가 있네요. 그 아래의 통계 정보들을 해석해볼까요?\n",
    "\n",
    "등장 빈도가 threshold 값인 7회 미만, 즉, 6회 이하인 단어들은 단어 집합에서 무려 70% 이상을 차지하네요. 하지만, 실제로 훈련 데이터에서 등장 빈도로 차지하는 비중은 상대적으로 적은 수치인 3.39%밖에 되지 않아요.\n",
    "\n",
    "그래서 등장 빈도가 6회 이하인 단어들은 정수 인코딩 과정에서 빼고, 훈련 데이터에서 제거하고자 합니다. 위에서 이를 제외한 단어 집합의 크기를 8,233으로 계산했는데, 이와 비슷한 값으로 어림잡아 단어 집합의 크기를 8000으로 제한해볼게요. 토크나이저를 정의할 때 num_words의 값을 정해주면, 단어 집합의 크기를 제한할 수 있어요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "changing-spokesman",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = 8000\n",
    "src_tokenizer = Tokenizer(num_words = src_vocab) # 단어 집합의 크기를 8,000으로 제한\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 단어 집합 재생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "therapeutic-details",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 909, 3104, 82, 624, 1790, 1581, 2772, 2042, 909], [569, 182, 6172, 363, 9, 1176, 2923, 129, 732, 654, 879, 1884, 22], [6, 42, 64, 33, 64, 393, 2207, 638, 79, 83, 138, 312, 97, 490, 167, 1095, 1113, 251, 163, 2812, 23, 14, 302]]\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train) \n",
    "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
    "\n",
    "#잘 진행되었는지 샘플 출력\n",
    "print(encoder_input_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "disciplinary-shuttle",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-indianapolis",
   "metadata": {},
   "source": [
    "이제 단어 집합이 생성되는 동시에 각 단어에 고유한 정수가 부여되었어요. 이는 tar_tokenizer.word_index에 저장되어있어요. tar_tokenizer.word_counts.items()에는 단어와 각 단어의 등장 빈도수가 저장되어져 있는데, 이를 통해서 통계적인 정보를 얻어서, 등장 빈도수가 6회 미만인 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인해볼게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "acquired-workshop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 10466\n",
      "등장 빈도가 5번 이하인 희귀 단어의 수: 8079\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 2387\n",
      "단어 집합에서 희귀 단어의 비율: 77.19281482897\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 5.844896203076262\n"
     ]
    }
   ],
   "source": [
    "threshold = 6\n",
    "total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tar_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-elite",
   "metadata": {},
   "source": [
    "등장 빈도가 5회 이하인 단어들은 단어 집합에서 약 77%를 차지하고있네요. \n",
    "하지만, 실제로 훈련 데이터에서 등장 빈도로 차지하는 비중은 상대적으로 매우 적은 \n",
    "수치인 5.89%밖에 되지 않아요. 아까 했던것과 동일하게 이 단어들은 모두 제거할게요. \n",
    "2,382에서 어림잡아 2,000을 단어 집합의 크기로 제한할게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "violent-madison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "input  [[1, 1111], [1, 724, 331, 131], [1, 1225, 1226], [1, 9, 116, 258, 76], [1, 44]]\n",
      "target\n",
      "decoder  [[1111, 2], [724, 331, 131, 2], [1225, 1226, 2], [9, 116, 258, 76, 2], [44, 2]]\n"
     ]
    }
   ],
   "source": [
    "tar_vocab = 2000\n",
    "tar_tokenizer = Tokenizer(num_words = tar_vocab) \n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train) \n",
    "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
    "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
    "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
    "\n",
    "#잘 변환되었는지 확인\n",
    "print('input')\n",
    "print('input ',decoder_input_train[:5])\n",
    "print('target')\n",
    "print('decoder ',decoder_target_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-adjustment",
   "metadata": {},
   "source": [
    "정상적으로 정수 인코딩 작업이 끝났어요. 현재 decoder_input_train과 decoder_target_train에는 더 이상 숫자 2,000이 넘는 숫자들은 존재하지 않아요. 그런데 다음 작업인 패딩하기로 넘어가기 전에 한 가지 점검해야할 것이 있어요.\n",
    "\n",
    "전체 데이터에서 빈도수가 낮은 단어가 삭제되었다는 것은 빈도수가 낮은 단어만으로 구성되었던 샘플들은 이제 빈(empty) 샘플이 되었을 가능성이 있어요. 이 현상은 길이가 상대적으로 길었던 원문(Text)의 경우에는 문제가 별로 없겠지만, 애초에 평균 길이가 4밖에 되지 않았던 요약문(Summary)의 경우에는 이 현상이 굉장히 두드러졌을 가능성이 높겠죠.\n",
    "\n",
    "요약문에서 길이가 0이 된 샘플들의 인덱스를 받아와볼게요. 여기서 주의할 점은 요약문인 decoder_input에는 sostoken 또는 decoder_target에는 eostoken이 추가된 상태이고, 이 두 토큰은 모든 샘플에서 등장하므로 빈도수가 샘플수와 동일하게 매우 높으므로 단어 집합 제한에도 삭제 되지 않아요. 그래서 이제 길이가 0이 된 요약문의 실제길이는 1로 나올거에요. 길이 0이 된 decoder_input에는 sostoken, decoder_target에는 eostoken만 남아 있을테니까요.\n",
    "\n",
    "훈련 데이터와 테스트 데이터에 대해서 요약문의 길이가 1인 경우의 인덱스를 각각 drop_train과 drop_test에 라는 변수에 저장해볼게요. 이 샘플들은 모두 삭제할거에요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "utility-passion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제할 훈련 데이터의 개수 : 1281\n",
      "삭제할 테스트 데이터의 개수 : 322\n",
      "훈련 데이터의 개수 : 51374\n",
      "훈련 레이블의 개수 : 51374\n",
      "테스트 데이터의 개수 : 12841\n",
      "테스트 레이블의 개수 : 12841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beanzsoft/.conda/envs/AIFFEL/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
    "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
    "\n",
    "print('삭제할 훈련 데이터의 개수 :',len(drop_train))\n",
    "print('삭제할 테스트 데이터의 개수 :',len(drop_test))\n",
    "\n",
    "encoder_input_train = np.delete(encoder_input_train, drop_train, axis=0)\n",
    "decoder_input_train = np.delete(decoder_input_train, drop_train, axis=0)\n",
    "decoder_target_train = np.delete(decoder_target_train, drop_train, axis=0)\n",
    "\n",
    "encoder_input_test = np.delete(encoder_input_test, drop_test, axis=0)\n",
    "decoder_input_test = np.delete(decoder_input_test, drop_test, axis=0)\n",
    "decoder_target_test = np.delete(decoder_target_test, drop_test, axis=0)\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :',len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :',len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :',len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-tradition",
   "metadata": {},
   "source": [
    "패딩하기\n",
    "텍스트 시퀀스를 정수 시퀀스로 변환했다면, 이제 서로 다른 길이의 샘플들을 병렬 처리하기 위해 같은 길이로 맞춰주는 패딩 작업을 해주어야 해야해요. 아까 정해두었던 최대 길이로 패딩해 줄 거에요. 최대 길이보다 짧은 데이터들은 뒤의 공간에 숫자 0을 넣어 최대 길이로 길이를 맞춰줄게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "settled-alpha",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = pad_sequences(encoder_input_train, maxlen = text_max_len, padding='post')\n",
    "encoder_input_test = pad_sequences(encoder_input_test, maxlen = text_max_len, padding='post')\n",
    "decoder_input_train = pad_sequences(decoder_input_train, maxlen = summary_max_len, padding='post')\n",
    "decoder_target_train = pad_sequences(decoder_target_train, maxlen = summary_max_len, padding='post')\n",
    "decoder_input_test = pad_sequences(decoder_input_test, maxlen = summary_max_len, padding='post')\n",
    "decoder_target_test = pad_sequences(decoder_target_test, maxlen = summary_max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "hazardous-component",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "# 인코더 설계 시작\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "\n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(text_max_len,))\n",
    "\n",
    "# 인코더의 임베딩 층\n",
    "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "# 인코더의 LSTM 1\n",
    "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# 인코더의 LSTM 2\n",
    "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# 인코더의 LSTM 3\n",
    "encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-thickness",
   "metadata": {},
   "source": [
    "임베딩 벡터의 차원은 128로 정의하고, hidden state의 크기를 256으로 정의했어요. hidden state는 LSTM에서 얼만큼의 수용력(capacity)를 가질지를 정하는 파라미터에요. 이 파라미터는 LSTM의 용량의 크기나, LSTM에서의 뉴론의 갯수라고 이해하면 돼요. 다른 신경망과 마찬가지로, 무조건 용량을 많이 준다고 해서 성능이 반드시 올라가는 것은 아니에요.\n",
    "\n",
    "인코더의 LSTM은 총 3개의 층으로 구성해서 모델의 복잡도를 높였어요. hidden state의 크기를 늘리는 것이 LSTM 층 1개의 용량을 늘린다면, 3개의 층을 사용하는 것은 모델의 용량을 늘린다고 볼 수 있죠. 3개의 층을 지나서 인코더로부터 나온 출력 벡터는 디코더로 보내줘야겠죠?\n",
    "\n",
    "디코더를 설계해볼게요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "regular-inquiry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 128)      1024000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 50, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    256000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 2000)   514000      lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,633,104\n",
      "Trainable params: 3,633,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 디코더 설계\n",
    "\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# 디코더의 임베딩 층\n",
    "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 디코더의 LSTM\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences = True, return_state = True, dropout = 0.4, recurrent_dropout=0.2)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state = [state_h, state_c])\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation = 'softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-winning",
   "metadata": {},
   "source": [
    "디코더의 출력층에서는 Summary의 단어장인 tar_vocab의 수많은 선택지 중 하나의 단어를 선택하는 다중 클래스 분류 문제를 풀어야 해요. 그렇기 때문에 Dense의 인자로 tar_vocab을 주고, 활성화 함수로 소프트맥스 함수를 사용하고 있어요.\n",
    "\n",
    "지금까지 설계한 것은 인코더의 hidden state와 cell state를 디코더의 초기 state로 사용하는 가장 기본적인 seq2seq에요. 그런데 디코더의 출력층을 설계를 살짝 바꿔서 성능을 높일 수 있는 방법이 있어요! 바로 어텐션 메커니즘이에요.\n",
    "\n",
    "어텐션 매커니즘\n",
    "어텐션 메커니즘을 수행하는 어텐션 함수를 설계하는 것은 또 다른 새로운 신경망을 설계해야한다는 뜻이에요. 어텐션 함수를 설계해보는 것은 다음 기회로 미루기로 하고, 여기서는 이미 구현된 어텐션 함수를 가져와서 디코더의 출력층에 어떤 방식으로 결합하는지 배워볼게요.\n",
    "\n",
    "아래의 코드를 수행하여 깃허브에 공개되어져 있는 어텐션 함수를 다운로드 할게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dietary-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/thushv89/attention_keras/master/src/layers/attention.py\", filename=\"attention.py\")\n",
    "from attention import AttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "improving-fishing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 128)      1024000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 50, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    256000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 256),  131328      lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 512)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 2000)   1026000     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 4,276,432\n",
      "Trainable params: 4,276,432\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 어텐션 층(어텐션 함수)\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "# 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
    "decoder_concat_input = Concatenate(axis = -1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "relative-tsunami",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51374 samples, validate on 12841 samples\n",
      "Epoch 1/50\n",
      "51374/51374 [==============================] - 1080s 21ms/sample - loss: 2.7062 - val_loss: 2.4071\n",
      "Epoch 2/50\n",
      "51374/51374 [==============================] - 817s 16ms/sample - loss: 2.3840 - val_loss: 2.2721\n",
      "Epoch 3/50\n",
      "51374/51374 [==============================] - 1543s 30ms/sample - loss: 2.2547 - val_loss: 2.1797\n",
      "Epoch 4/50\n",
      "51374/51374 [==============================] - 49467s 963ms/sample - loss: 2.1578 - val_loss: 2.0986\n",
      "Epoch 5/50\n",
      "51374/51374 [==============================] - 831s 16ms/sample - loss: 2.0795 - val_loss: 2.0427\n",
      "Epoch 6/50\n",
      "51374/51374 [==============================] - 832s 16ms/sample - loss: 2.0195 - val_loss: 2.0119\n",
      "Epoch 7/50\n",
      "51374/51374 [==============================] - 837s 16ms/sample - loss: 1.9744 - val_loss: 1.9769\n",
      "Epoch 8/50\n",
      "51374/51374 [==============================] - 873s 17ms/sample - loss: 1.9350 - val_loss: 1.9532\n",
      "Epoch 9/50\n",
      "51374/51374 [==============================] - 837s 16ms/sample - loss: 1.8995 - val_loss: 1.9294\n",
      "Epoch 10/50\n",
      "51374/51374 [==============================] - 1167s 23ms/sample - loss: 1.8676 - val_loss: 1.9101\n",
      "Epoch 11/50\n",
      "51374/51374 [==============================] - 878s 17ms/sample - loss: 1.8387 - val_loss: 1.8964\n",
      "Epoch 12/50\n",
      "51374/51374 [==============================] - 876s 17ms/sample - loss: 1.8116 - val_loss: 1.8872\n",
      "Epoch 13/50\n",
      "51374/51374 [==============================] - 1058s 21ms/sample - loss: 1.7875 - val_loss: 1.8781\n",
      "Epoch 14/50\n",
      "51374/51374 [==============================] - 922s 18ms/sample - loss: 1.7643 - val_loss: 1.8683\n",
      "Epoch 15/50\n",
      "51374/51374 [==============================] - 944s 18ms/sample - loss: 1.7423 - val_loss: 1.8639\n",
      "Epoch 16/50\n",
      "51374/51374 [==============================] - 1047s 20ms/sample - loss: 1.7229 - val_loss: 1.8517\n",
      "Epoch 17/50\n",
      "51374/51374 [==============================] - 917s 18ms/sample - loss: 1.7036 - val_loss: 1.8478\n",
      "Epoch 18/50\n",
      "51374/51374 [==============================] - 963s 19ms/sample - loss: 1.6855 - val_loss: 1.8440\n",
      "Epoch 19/50\n",
      "51374/51374 [==============================] - 876s 17ms/sample - loss: 1.6690 - val_loss: 1.8425\n",
      "Epoch 20/50\n",
      "51374/51374 [==============================] - 1227s 24ms/sample - loss: 1.6533 - val_loss: 1.8354\n",
      "Epoch 21/50\n",
      "51374/51374 [==============================] - 1524s 30ms/sample - loss: 1.6382 - val_loss: 1.8376\n",
      "Epoch 22/50\n",
      "51374/51374 [==============================] - 880s 17ms/sample - loss: 1.6245 - val_loss: 1.8357\n",
      "Epoch 00022: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 2)\n",
    "history = model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_target_train, \\\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size = 256, callbacks=[es], epochs = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-software",
   "metadata": {},
   "source": [
    "EarlyStopping은 한국어로 해석 하면 '조기 종료'의 뜻을 가지고 있는데, 특정 조건이 충족되면 모델의 훈련을 멈추는 역할을 해요. 여기서는 val_loss(검증 데이터의 손실)을 모니터링 하면서, 검증 데이터의 손실이 줄어들지 않고 증가하는 현상이patiensce =2 2회 관측되면 학습을 멈추도록 설정되어져 있어요. 26번째 epoch쯤에서 조기종료되네요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fantastic-mistake",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "rough-roller",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
    "tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
    "tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-checklist",
   "metadata": {},
   "source": [
    "seq2seq는 훈련할 때와 실제 동작할 때(인퍼런스 단계)의 방식이 다르므로 그에 맞게 모델 설계를 별개로 진행해야 한다는 것, 알고 계시나요?\n",
    "\n",
    "훈련 단계에서는 디코더의 입력부에 정답이 되는 문장 전체를 한꺼번에 넣고 디코더의 출력과 한번에 비교할 수 있으므로, 인코더와 디코더를 엮은 통짜 모델 하나만 준비했습니다.\n",
    "\n",
    "그러나 정답 문장이 없는 인퍼런스 단계에서는 만들어야 할 문장의 길이만큼 디코더가 반복 구조로 동작해야 하기 때문에 부득이하게 인퍼런스를 위한 모델 설계를 별도로 해주어야 합니다. 이때는 인코더 모델과 디코더 모델을 분리해서 설계합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "sixth-reaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
    "tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
    "tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음\n",
    "\n",
    "# 인코더 설계\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "corrected-surprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어텐션 함수\n",
    "decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat) \n",
    "\n",
    "# 최종 디코더 모델\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "parallel-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "     # <SOS>에 해당하는 토큰 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_word_to_index['sostoken']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n",
    "\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = tar_index_to_word[sampled_token_index]\n",
    "\n",
    "        if(sampled_token!='eostoken'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        #  <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_token == 'eostoken'  or len(decoded_sentence.split()) >= (summary_max_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 길이가 1인 타겟 시퀀스를 업데이트\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 상태를 업데이트 합니다.\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "statutory-angel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 : recently came across delicous soda supermarket really refreshing wonderful little shot cucumber infused gin however price ridiculous ashamed selling crazy high price get market bottle \n",
      "실제 요약 : delicous \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : gluten free bar much better rich combination says fruit nuts however unit description incorrect boxes one box eight bars \n",
      "실제 요약 : very tasty \n",
      "예측 요약 :  not gluten free\n",
      "\n",
      "\n",
      "원문 : excellent product yummy wholesome also good price lower local natural food stores \n",
      "실제 요약 : yummy and wholesome \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : tasty salty price disappointing many shells empty expected find pistachios bottom bag assuming fell shells package buy product \n",
      "실제 요약 : empty shells \n",
      "예측 요약 :  not bad\n",
      "\n",
      "\n",
      "원문 : product good love taste tea easy use hot cold \n",
      "실제 요약 : awesome product \n",
      "예측 요약 :  great tea\n",
      "\n",
      "\n",
      "원문 : love drink instant requires little extra stirring mix completely water shake stir well granules stay bottom well caffeine none green tea acai honey good ingredients top low calories five per packet strawberry goes well acai really refreshing drink drink totally guilt free \n",
      "실제 요약 : love love love it \n",
      "예측 요약 :  good stuff\n",
      "\n",
      "\n",
      "원문 : definitely buying husband love feels good know pronounce also \n",
      "실제 요약 : better than expected \n",
      "예측 요약 :  great\n",
      "\n",
      "\n",
      "원문 : outstanding snack super fast lives review saw food network make sure shake pkg really good microwaving super \n",
      "실제 요약 : best snack ever \n",
      "예측 요약 :  great snack\n",
      "\n",
      "\n",
      "원문 : find better loose green tea price able get free shipping well since bought something else tea used many japanese people fall high priced organic green teas produced american companies good getting anything better cha tea \n",
      "실제 요약 : great price great quality \n",
      "예측 요약 :  great tea\n",
      "\n",
      "\n",
      "원문 : bought wild planet four package amazon tasty filling love fact cans bpa free meaty chunk salmon eating bee canned salmon still prefer taste would close second \n",
      "실제 요약 : high quality salmon much better than other brands \n",
      "예측 요약 :  good but not great\n",
      "\n",
      "\n",
      "원문 : third review eden organic product say pleased continue keep hand satisfied customer \n",
      "실제 요약 : another staple on the shelf \n",
      "예측 요약 :  good product\n",
      "\n",
      "\n",
      "원문 : product excellent really help dog however price high little amount received found much bigger bag locally better rate would buy though \n",
      "실제 요약 : review \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : looking proper nutrition cereal one best period low sugar calorie high fiber protein one ask add low fat milk frozen fruit perfect breakfast wish could buy bulk \n",
      "실제 요약 : the best cereal in of nutrition \n",
      "예측 요약 :  great cereal\n",
      "\n",
      "\n",
      "원문 : received order today got one pack popcorn paid case got one pack would rate zero would let \n",
      "실제 요약 : what the \n",
      "예측 요약 :  popcorn\n",
      "\n",
      "\n",
      "원문 : recently gifted first bottle pepper plant original ca style hot sauce good chili nose opening bottle chili flavor present followed garlic balanced heat flavors vinegary salty reminds sriracha makes good everyday table sauce \n",
      "실제 요약 : flavor \n",
      "예측 요약 :  the best\n",
      "\n",
      "\n",
      "원문 : ryvita tried filling taste great eat absolutely guilt calories per fat sugar eat three completely satisfied without blown diet eat fruit afternoon snack perfectly balance sugar hit get fruit keep going hours \n",
      "실제 요약 : good stuff \n",
      "예측 요약 :  great taste\n",
      "\n",
      "\n",
      "원문 : best tea ever tasted love tazo teas think come sachets instead tea bags sachets actually make kinds course products make \n",
      "실제 요약 : awesome \n",
      "예측 요약 :  great tea\n",
      "\n",
      "\n",
      "원문 : use primarily environment around work well family much convenient pack trips give children apply always keep around buy long keep selling bought amazon local stores longer selling \n",
      "실제 요약 : why cannot find this in stores anymore \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : started eating bulgur instead rice great deal bulgur great quality delicious totally recommend company order \n",
      "실제 요약 : economical and great quality \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : one best flavored teas tried \n",
      "실제 요약 : awesome flavored tea \n",
      "예측 요약 :  great tea\n",
      "\n",
      "\n",
      "원문 : happy found tea amazon looking stores year shipped immediately happy kitchen \n",
      "실제 요약 : very good \n",
      "예측 요약 :  great tea\n",
      "\n",
      "\n",
      "원문 : really good single guy easier meal better super simple make tasted wonderful like velveeta skillet dinners love also tried broccoli flavor great glad tried product makes dinner easy \n",
      "실제 요약 : yum \n",
      "예측 요약 :  great for lunch\n",
      "\n",
      "\n",
      "원문 : great barbecue flavor husband went entire six pack month tried really truly nice smokey barbecue flavor came buy per request sadly says longer hubby upset please bring back thank god bless \n",
      "실제 요약 : bring this back amazon \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : good price shipping makes buy \n",
      "실제 요약 : good price but \n",
      "예측 요약 :  great price\n",
      "\n",
      "\n",
      "원문 : made froze pull eat still moist good flavor since chocolate \n",
      "실제 요약 : bran muffins \n",
      "예측 요약 :  good stuff\n",
      "\n",
      "\n",
      "원문 : bars best health bars ever eaten moist sweet previous favorite kind almond apricot think could wrong mango macadamia slight find craving bars time kind company bars yummy good could better \n",
      "실제 요약 : mouth addictive \n",
      "예측 요약 :  great snack\n",
      "\n",
      "\n",
      "원문 : bare fruit organic bake dried apples healthy well delicious delivered quarter \n",
      "실제 요약 : healthy \n",
      "예측 요약 :  great snack\n",
      "\n",
      "\n",
      "원문 : love product perfect school lunches little guy loves ingredients wholesome \n",
      "실제 요약 : wow \n",
      "예측 요약 :  great snack\n",
      "\n",
      "\n",
      "원문 : tastes great makes nice flavor veggies simple soup use frequently booster vita mix extra flavor may super healthy like cant stand reduced sodium low fat canned soup flavorless give shot think would pleasantly surprised \n",
      "실제 요약 : worth look \n",
      "예측 요약 :  good stuff\n",
      "\n",
      "\n",
      "원문 : yerba mate naturally caffeinated rainforest one could call breakfast tea earl grey amazon smoky flavor rainforest adding cause delicious tea yerba mate perfect accompaniment meal especially breakfast rainforest themed cafe comforting complete comforting energizing tea enjoy cuppa yerba mate \n",
      "실제 요약 : taste of the rainforest \n",
      "예측 요약 :  great tea\n",
      "\n",
      "\n",
      "원문 : boyfriend tried gummy bears favorite frozen yogurt place said best gummy bears ever tell kind found amazon bought lb bag haha wanted go get frozen yogurt every day gummy bears anyway really flavorful gummy bears \n",
      "실제 요약 : best gummy bears ever \n",
      "예측 요약 :  haribo gummi bears\n",
      "\n",
      "\n",
      "원문 : purchased soap skin dry sensitive much face neck get soap skin even skin feel tight uncomfortable need use lot lotion afterwards make thing works skin bag highly recommend may sticky smell strange smells better olive oil stuff always works go bag \n",
      "실제 요약 : horrible smell even worse feeling \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : kids love vanilla think taste great nice find small bagged snack contain gm ingredients tastes good \n",
      "실제 요약 : love these \n",
      "예측 요약 :  great snack\n",
      "\n",
      "\n",
      "원문 : bought bigelow green tea would advise anyone purchasing product read country origin product china write says product blended distributed usa deceptive country listed food products according food drug required actually list product comes \n",
      "실제 요약 : deceptive \n",
      "예측 요약 :  not bad\n",
      "\n",
      "\n",
      "원문 : got small gift girlfriend always buy cheap popcorn store thought would enjoy unique gourmet popcorn right really liked especially purple kind cook stove oil worked perfectly however like splash oil face feel good popcorn great distinct flavor think returning \n",
      "실제 요약 : tasty \n",
      "예측 요약 :  great popcorn\n",
      "\n",
      "\n",
      "원문 : bought daughter party well strawberry loves things great product \n",
      "실제 요약 : great product \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : pasta made del italy since love italian food severe digestive allergic problems pasta friendly tastes better pasta eaten lot one favorite foods \n",
      "실제 요약 : best pasta and its organic \n",
      "예측 요약 :  great pasta\n",
      "\n",
      "\n",
      "원문 : really wanted good bland taste nothing like potatoes actually taste like rice krispies cereal want something tastes like potato chip bout low calories buy popchip popchips original ounce single serve bags \n",
      "실제 요약 : rice on \n",
      "예측 요약 :  tasty but not the best\n",
      "\n",
      "\n",
      "원문 : far favorite best chocolate bar ever eaten yum \n",
      "실제 요약 : best chocolate bar ever \n",
      "예측 요약 :  delicious\n",
      "\n",
      "\n",
      "원문 : good lots ginger spice although find bit much people like \n",
      "실제 요약 : lots of ginger \n",
      "예측 요약 :  great taste\n",
      "\n",
      "\n",
      "원문 : corn company adults good kids appreciate shipping little steep used special events well worth price light fragile corn virtually hulls good quality \n",
      "실제 요약 : for company \n",
      "예측 요약 :  not as good as the\n",
      "\n",
      "\n",
      "원문 : organic way go newman organics turkey formula cats ounce cans \n",
      "실제 요약 : organic is the only way to go \n",
      "예측 요약 :  my cats love this\n",
      "\n",
      "\n",
      "원문 : keep review short simple saying month old kitten loves food recently bought pack gets extremely excited time hears crack open highly recommended \n",
      "실제 요약 : delicious \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : good cookies dry bland certainly worth calories high fiber healthy cookie per se nothing recommend cookies \n",
      "실제 요약 : disappointing \n",
      "예측 요약 :  not bad\n",
      "\n",
      "\n",
      "원문 : cannot get son eat anything else always eat tastes delicious mix baby cereal formula \n",
      "실제 요약 : delicious \n",
      "예측 요약 :  great snack\n",
      "\n",
      "\n",
      "원문 : nana gluten cookie bars berry vanilla good chocolate munch flavor good moist good flavor little crunch reason giving four stars instead five texture little bit crumbly buying \n",
      "실제 요약 : very good \n",
      "예측 요약 :  great cookies\n",
      "\n",
      "\n",
      "원문 : remember shake bake years ago quick easy use pork comes moist tender grand kids enjoyed shaking pork chops even mild flavor even sprinkled remaining crumbs top chops even comes variety flavors \n",
      "실제 요약 : shake bake pork pouch ounce \n",
      "예측 요약 :  great for cooking\n",
      "\n",
      "\n",
      "원문 : quite sure cats would eat never fed anything dry kibble need worried cats love like human grade ingredients contains absolutely gluten something know ordering petite cuisine comes recyclable aluminum cans means contribute landfill problem complaint cost probably buy cats special treat love \n",
      "실제 요약 : my cats love this stuff \n",
      "예측 요약 :  my cats love this\n",
      "\n",
      "\n",
      "원문 : trying every type cup get hands since got keurig christmas encountered truly hate discovered one would though pretty difficult ruin hot chocolate people grove square certainly good job hot chocolate watery horrible plastic aftertaste also yuck \n",
      "실제 요약 : mm plastic \n",
      "예측 요약 :  not the best\n",
      "\n",
      "\n",
      "원문 : daughter introduced june difficulty finding stores decided use favorite amazon locate got great price tell comparison tea full bodied delicious would highly recommend tea drinker \n",
      "실제 요약 : no \n",
      "예측 요약 :  delicious\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2text(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            temp = temp + src_index_to_word[i]+' '\n",
    "    return temp\n",
    "\n",
    "# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2summary(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=tar_word_to_index['sostoken']) and i!=tar_word_to_index['eostoken']):\n",
    "            temp = temp + tar_index_to_word[i] + ' '\n",
    "    return temp\n",
    "\n",
    "for i in range(50, 100):\n",
    "    print(\"원문 :\", seq2text(encoder_input_test[i]))\n",
    "    print(\"실제 요약 :\", seq2summary(decoder_input_test[i]))\n",
    "    print(\"예측 요약 :\", decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-purchase",
   "metadata": {},
   "source": [
    "11-12. 추출적 요약 해보기\n",
    "앞서 seq2seq를 통해서 추상적 요약을 진행해봤어요. 그런데 텍스트 요약에는 추상적 요약 외에도 이미 본문에 존재하는 단어구, 문장을 뽑아서 요약으로 삼는 추출적 요약 방법도 있었죠.\n",
    "\n",
    "패키지 Summa에서는 추출적 요약을 위한 모듈인 summarize를 제공하고 있어 아주 간단하게 실습을 해볼 수 있어요. 영화 매트릭스 시놉시스를 요약해보면서 summarize 사용법을 익혀볼까요?\n",
    "\n",
    "패키지 설치\n",
    "먼저 필요한 패키지를 아래와 같이 설치해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-saudi",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from summa.summarizer import summarize\n",
    "\n",
    "text = requests.get('http://rare-technologies.com/the_matrix_synopsis.txt').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-counter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
