{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "E15_Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRRJLHsYN2CT"
      },
      "source": [
        "15-14. 프로젝트: 한국어 데이터로 챗봇 만들기\n",
        "영어로 만들었던 챗봇을 한국어 데이터로 바꿔서 훈련시켜봅시다.\n",
        "\n",
        "\n",
        "Step 3. SubwordTextEncoder 사용하기\n",
        "한국어 데이터는 형태소 분석기를 사용하여 토크나이징을 해야 한다고 많은 분이 알고 있습니다. 하지만 여기서는 형태소 분석기가 아닌 위 실습에서 사용했던 내부 단어 토크나이저인 SubwordTextEncoder를 그대로 사용해보세요.\n",
        "\n",
        "Step 4. 모델 구성하기\n",
        "위 실습 내용을 참고하여 트랜스포머 모델을 구현합니다.\n",
        "\n",
        "Step 5. 모델 평가하기\n",
        "Step 1에서 선택한 전처리 방법을 고려하여 입력된 문장에 대해서 대답을 얻는 예측 함수를 만듭니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT0JIYjlN8Gz"
      },
      "source": [
        "Step 1. 데이터 수집하기\n",
        "한국어 챗봇 데이터는 송영숙님이 공개한 챗봇 데이터를 사용합니다.\n",
        "\n",
        "이 데이터는 아래의 링크에서 다운로드할 수 있습니다.\n",
        "\n",
        "songys/Chatbot_data\n",
        "wget으로 데이터 다운로드\n",
        "$ wget https://github.com/songys/Chatbot_data/raw/master/ChatbotData%20.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVC-4-A-OCwW"
      },
      "source": [
        "data_path = '/content/drive/MyDrive/DL_Study/AIFFEL/chat_bot/ChatbotData .csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRm6vBiFO54T"
      },
      "source": [
        "\n",
        "Step 2. 데이터 전처리하기\n",
        "영어 데이터와는 전혀 다른 데이터인 만큼 영어 데이터에 사용했던 전처리와 일부 동일한 전처리도 필요하겠지만 전체적으로는 다른 전처리를 수행해야 할 수도 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aUM4gJ_O3IN"
      },
      "source": [
        "# 전처리 함수\n",
        "def preprocess_sentence(sentence):\n",
        "  sentence = sentence.lower().strip()\n",
        "\n",
        "  # 단어와 구두점(punctuation) 사이의 거리를 만듭니다.\n",
        "  # 예를 들어서 \"I am a student.\" => \"I am a student .\"와 같이\n",
        "  # student와 온점 사이에 거리를 만듭니다.\n",
        "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "\n",
        "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\")를 제외한 모든 문자를 공백인 ' '로 대체합니다.\n",
        "  sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
        "  sentence = sentence.strip()\n",
        "  return sentence\n",
        "print(\"슝=3\")\n",
        "\n",
        "# 질문과 답변의 쌍인 데이터셋을 구성하기 위한 데이터 로드 함수\n",
        "def load_conversations():\n",
        "  id2line = {}\n",
        "  with open(path_to_movie_lines, errors='ignore') as file:\n",
        "    lines = file.readlines()\n",
        "  for line in lines:\n",
        "    parts = line.replace('\\n', '').split(' +++$+++ ')\n",
        "    id2line[parts[0]] = parts[4]\n",
        "\n",
        "  inputs, outputs = [], []\n",
        "  with open(path_to_movie_conversations, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "  for line in lines:\n",
        "    parts = line.replace('\\n', '').split(' +++$+++ ')\n",
        "    conversation = [line[1:-1] for line in parts[3][1:-1].split(', ')]\n",
        "\n",
        "    for i in range(len(conversation) - 1):\n",
        "\t\t\t# 전처리 함수를 질문에 해당되는 inputs와 답변에 해당되는 outputs에 적용.\n",
        "      inputs.append(preprocess_sentence(id2line[conversation[i]]))\n",
        "      outputs.append(preprocess_sentence(id2line[conversation[i + 1]]))\n",
        "\n",
        "      if len(inputs) >= MAX_SAMPLES:\n",
        "        return inputs, outputs\n",
        "  return inputs, outputs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}