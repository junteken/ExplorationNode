{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names'])\n     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n0      14.23        1.71  2.43               15.6      127.0           2.80   \n1      13.20        1.78  2.14               11.2      100.0           2.65   \n2      13.16        2.36  2.67               18.6      101.0           2.80   \n3      14.37        1.95  2.50               16.8      113.0           3.85   \n4      13.24        2.59  2.87               21.0      118.0           2.80   \n..       ...         ...   ...                ...        ...            ...   \n173    13.71        5.65  2.45               20.5       95.0           1.68   \n174    13.40        3.91  2.48               23.0      102.0           1.80   \n175    13.27        4.28  2.26               20.0      120.0           1.59   \n176    13.17        2.59  2.37               20.0      120.0           1.65   \n177    14.13        4.10  2.74               24.5       96.0           2.05   \n\n     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n0          3.06                  0.28             2.29             5.64  1.04   \n1          2.76                  0.26             1.28             4.38  1.05   \n2          3.24                  0.30             2.81             5.68  1.03   \n3          3.49                  0.24             2.18             7.80  0.86   \n4          2.69                  0.39             1.82             4.32  1.04   \n..          ...                   ...              ...              ...   ...   \n173        0.61                  0.52             1.06             7.70  0.64   \n174        0.75                  0.43             1.41             7.30  0.70   \n175        0.69                  0.43             1.35            10.20  0.59   \n176        0.68                  0.53             1.46             9.30  0.60   \n177        0.76                  0.56             1.35             9.20  0.61   \n\n     od280/od315_of_diluted_wines  proline  \n0                            3.92   1065.0  \n1                            3.40   1050.0  \n2                            3.17   1185.0  \n3                            3.45   1480.0  \n4                            2.93    735.0  \n..                            ...      ...  \n173                          1.74    740.0  \n174                          1.56    750.0  \n175                          1.56    835.0  \n176                          1.62    840.0  \n177                          1.60    560.0  \n\n[178 rows x 13 columns]\nX_train 개수:  142 , X_test 개수:  36\nDecision tree결과\nclassifier\naccuracy = 0.9444444444444444\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00         7\n           1       0.89      1.00      0.94        17\n           2       1.00      0.83      0.91        12\n\n    accuracy                           0.94        36\n   macro avg       0.96      0.94      0.95        36\nweighted avg       0.95      0.94      0.94        36\n\nRandom forest결과\nclassifier\naccuracy = 1.0\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00         7\n           1       1.00      1.00      1.00        17\n           2       1.00      1.00      1.00        12\n\n    accuracy                           1.00        36\n   macro avg       1.00      1.00      1.00        36\nweighted avg       1.00      1.00      1.00        36\n\nSVM 결과\nclassifier\naccuracy = 0.6111111111111112\n              precision    recall  f1-score   support\n\n           0       0.86      0.86      0.86         7\n           1       0.58      0.88      0.70        17\n           2       0.33      0.08      0.13        12\n\n    accuracy                           0.61        36\n   macro avg       0.59      0.61      0.56        36\nweighted avg       0.55      0.61      0.54        36\n\nSGD Classifier 결과\nclassifier\naccuracy = 0.5555555555555556\n              precision    recall  f1-score   support\n\n           0       1.00      0.43      0.60         7\n           1       0.52      1.00      0.68        17\n           2       0.00      0.00      0.00        12\n\n    accuracy                           0.56        36\n   macro avg       0.51      0.48      0.43        36\nweighted avg       0.44      0.56      0.44        36\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "\n",
    "def train_and_print(clssifier, x_train, y_train, x_test, y_test):\n",
    "    print(clssifier._estimator_type)\n",
    "    clssifier.fit(x_train, y_train)\n",
    "    y_pred = clssifier.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('accuracy = '+str(accuracy))    \n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "def sklearn_sample_practice(data, label):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, \n",
    "                                                    label, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=7)\n",
    "\n",
    "    print('X_train 개수: ', len(x_train), ', X_test 개수: ', len(x_test))\n",
    "    \n",
    "    decision_tree = DecisionTreeClassifier(random_state=32)\n",
    "    print('Decision tree결과')\n",
    "    train_and_print(decision_tree,x_train, y_train, x_test, y_test)\n",
    "\n",
    "    random_forest = RandomForestClassifier(random_state=32)\n",
    "    print('Random forest결과')\n",
    "    train_and_print(random_forest,x_train, y_train, x_test, y_test)\n",
    "\n",
    "    ml = svm.SVC()\n",
    "    print('SVM 결과')\n",
    "    train_and_print(ml,x_train, y_train, x_test, y_test)\n",
    "\n",
    "    sgd_model = SGDClassifier()\n",
    "    print('SGD Classifier 결과')\n",
    "    train_and_print(sgd_model,x_train, y_train, x_test, y_test)\n",
    "\n",
    "'''\n",
    "iris = load_iris()\n",
    "iris_data = iris.data\n",
    "iris_label = iris.target\n",
    "\n",
    "sklearn_sample_practice(iris_data, iris_label)\n",
    "\n",
    "#digit문제 시작\n",
    "print('digits 문제 결과')\n",
    "\n",
    "digits = load_digits()\n",
    "digits.keys()\n",
    "digit_data= digits.data\n",
    "digit_label= digits.target\n",
    "\n",
    "sklearn_sample_practice(digit_data, digit_label)\n",
    "'''\n",
    "#load wine문제\n",
    "wine= load_wine()\n",
    "print(wine.keys())\n",
    "wine_data= wine.data\n",
    "wine_label= wine.target\n",
    "iris_df = pd.DataFrame(data=wine_data, columns=wine.feature_names)\n",
    "print(iris_df)\n",
    "\n",
    "sklearn_sample_practice(wine_data, wine_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}