{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GD_4_Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4-zrKEYZQvP"
      },
      "source": [
        "from tensorflow.keras.datasets import reuters\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.naive_bayes import MultinomialNB #다항분포 나이브 베이즈 모델\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score #정확도 계산\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "def makeDtmTfIdf(x_train, x_test):\n",
        "  # train dtm vector만들기\n",
        "  dtmvector = CountVectorizer()\n",
        "  x_train_dtm = dtmvector.fit_transform(x_train)\n",
        "  tfidf_transformer = TfidfTransformer()\n",
        "  tfidfv = tfidf_transformer.fit_transform(x_train_dtm)\n",
        "\n",
        "  x_test_dtm = dtmvector.transform(x_test) #테스트 데이터를 DTM으로 변환\n",
        "  tfidfv_test = tfidf_transformer.transform(x_test_dtm) #DTM을 TF-IDF 행렬로 변환\n",
        "  \n",
        "  return tfidfv, tfidfv_test\n",
        "\n",
        "def MultinomialNaiveBayesClassifier(x_train, y_train, x_test, y_test):\n",
        "  \n",
        "  tfidfv, tfidfv_test = makeDtmTfIdf(x_train, x_test)\n",
        "  mod = MultinomialNB()\n",
        "  mod.fit(tfidfv, y_train)\n",
        "  predicted = mod.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
        "  print(\"Multinomial NB 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
        "\n",
        "def ComplementNaiveBayesClasifier(x_train, y_train, x_test, y_test):\n",
        "  tfidfv, tfidfv_test = makeDtmTfIdf(x_train, x_test)\n",
        "  cb = ComplementNB()\n",
        "  cb.fit(tfidfv, y_train)\n",
        "  predicted = cb.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
        "  print(\"Complement Naive Bayes 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
        "\n",
        "\n",
        "def LogisticRegressionClassifier(x_train, y_train, x_test, y_test):\n",
        "  tfidfv, tfidfv_test = makeDtmTfIdf(x_train, x_test)\n",
        "\n",
        "  lr = LogisticRegression(C=10000, penalty='l2')\n",
        "  lr.fit(tfidfv, y_train)\n",
        "  predicted = lr.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
        "  print(\"Logistic Regression 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
        "\n",
        "def LinearSVM(x_train, y_train, x_test, y_test):\n",
        "  tfidfv, tfidfv_test = makeDtmTfIdf(x_train, x_test)\n",
        "\n",
        "  lsvc = LinearSVC(C=1000, penalty='l1', max_iter=500, dual=False)\n",
        "  lsvc.fit(tfidfv, y_train)\n",
        "\n",
        "  predicted = lsvc.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
        "  print(\"LinearSVM 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
        "\n",
        "def DTClassifier(x_train, y_train, x_test, y_test):\n",
        "  tree = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
        "  tfidfv, tfidfv_test = makeDtmTfIdf(x_train, x_test)\n",
        "  tree.fit(tfidfv, y_train)\n",
        "  predicted = tree.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
        "  print(\"Decesion Tree Classifier 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
        "\n",
        "def RFClassifier(x_train, y_train, x_test, y_test):\n",
        "  tfidfv, tfidfv_test = makeDtmTfIdf(x_train, x_test)\n",
        "  forest = RandomForestClassifier(n_estimators=5, random_state=0)\n",
        "  forest.fit(tfidfv, y_train)\n",
        "  predicted = forest.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
        "  print(\"RandomForest 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
        "\n",
        "def GBClassifier(x_train, y_train, x_test, y_test):\n",
        "  tfidfv, tfidfv_test = makeDtmTfIdf(x_train, x_test)\n",
        "  grbt = GradientBoostingClassifier(random_state=0) # verbose=3\n",
        "  grbt.fit(tfidfv, y_train)\n",
        "  predicted = grbt.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
        "  print(\"Gradient Boosting 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
        "\n",
        "def VTClassifier(x_train, y_train, x_test, y_test):\n",
        "  tfidfv, tfidfv_test = makeDtmTfIdf(x_train, x_test)\n",
        "  voting_classifier = VotingClassifier(estimators=[\n",
        "         ('lr', LogisticRegression(C=10000, penalty='l2')),\n",
        "        ('cb', ComplementNB()),\n",
        "        ('grbt', GradientBoostingClassifier(random_state=0))\n",
        "        ], voting='soft', n_jobs=-1)\n",
        "  voting_classifier.fit(tfidfv, y_train)\n",
        "\n",
        "  predicted = voting_classifier.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
        "  print(\"Voting Classifier 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
        "\n",
        "def RunAllClasifier(x_train, y_train, x_test, y_test):\n",
        "  \n",
        "  cf_list= [MultinomialNaiveBayesClassifier, ComplementNaiveBayesClasifier, \n",
        "            LogisticRegressionClassifier, LinearSVM, DTClassifier, RFClassifier, \n",
        "            GBClassifier]\n",
        "  for f in cf_list:\n",
        "    f(x_train, y_train, x_test, y_test)\n",
        "\n",
        "\n",
        "\n",
        "def AddSpecialIndex(index_to_word, data):\n",
        "\n",
        "  decoded = []\n",
        "  for i in range(len(data)):\n",
        "    t = ' '.join([index_to_word[index] for index in data[i]])\n",
        "    decoded.append(t)\n",
        "\n",
        "  return decoded\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80T_VPBPZDI9"
      },
      "source": [
        "# 1. 모든 단어 사용\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R10oOrFxYw6d",
        "outputId": "1cf653ee-dc0f-4593-97f9-f2730361dc2d"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)\n",
        "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
        "index_to_word = {index + 3 : word for word, index in word_index.items()}\n",
        "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
        "    index_to_word[index]=token\n",
        "\n",
        "x_train = AddSpecialIndex(index_to_word, x_train)\n",
        "x_test = AddSpecialIndex(index_to_word, x_test)\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYFpe3NLgcc4",
        "outputId": "c4430d7c-6fc2-46ec-d5c1-ee35ba247a82"
      },
      "source": [
        "RunAllClasifier(x_train, y_train, x_test, y_test)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Multinomial NB 정확도: 0.5997328584149599\n",
            "Complement Naive Bayes 정확도: 0.7649154051647373\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression 정확도: 0.813446126447017\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LinearSVM 정확도: 0.780053428317008\n",
            "Decesion Tree Classifier 정확도: 0.6211041852181657\n",
            "RandomForest 정확도: 0.6544968833481746\n",
            "Gradient Boosting 정확도: 0.7684772929652716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VPqWq7rrqBr"
      },
      "source": [
        "# 빈도수 상위 5000개의 단어만 사용\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bZNK-zNgwiO",
        "outputId": "fdc3bd3a-75f5-4915-cf2d-fc378ef4f27a"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=5000, test_split=0.2)\n",
        "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
        "index_to_word = {index + 3 : word for word, index in word_index.items()}\n",
        "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
        "    index_to_word[index]=token\n",
        "\n",
        "x_train = AddSpecialIndex(index_to_word, x_train)\n",
        "x_test = AddSpecialIndex(index_to_word, x_test)\n",
        "RunAllClasifier(x_train, y_train, x_test, y_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Multinomial NB 정확도: 0.6731967943009796\n",
            "Complement Naive Bayes 정확도: 0.7707034728406055\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19dtSj3_s71C"
      },
      "source": [
        "# 딥러닝 모델 적용하기 (LSTM 사용)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1H6zqT1smDl"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "vocab_size = \n",
        "\n",
        "def train_LSTM(X_train, y_train):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, 100))\n",
        "    model.add(LSTM(128))\n",
        "    model.add(Dense(46, activation='sigmoid')) # 46개의 class를 분류해야한다.\n",
        "\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "    mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "    model.compile(optimizer='rmsprop', loss='CategoricalCrossentropy', metrics=['acc'])\n",
        "    history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)\n",
        "\n",
        "    return model, history"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}