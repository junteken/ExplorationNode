{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GD_4_Project.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4-zrKEYZQvP"
      },
      "source": [
        "from tensorflow.keras.datasets import reuters\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.naive_bayes import MultinomialNB #다항분포 나이브 베이즈 모델\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score #정확도 계산\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "def makeDtmTfIdf(x_train, x_test):\n",
        "  # train dtm vector만들기\n",
        "  dtmvector = CountVectorizer()\n",
        "  x_train_dtm = dtmvector.fit_transform(x_train)\n",
        "  tfidf_transformer = TfidfTransformer()\n",
        "  tfidfv = tfidf_transformer.fit_transform(x_train_dtm)\n",
        "\n",
        "  x_test_dtm = dtmvector.transform(x_test) #테스트 데이터를 DTM으로 변환\n",
        "  tfidfv_test = tfidf_transformer.transform(x_test_dtm) #DTM을 TF-IDF 행렬로 변환\n",
        "  \n",
        "  return tfidfv, tfidfv_test\n",
        "\n",
        "def MultinomialNaiveBayesClassifier(x_train, y_train, x_test, y_test):\n",
        "  \n",
        "  tfidfv, tfidfv_test = makeDtmTfIdf(x_train, x_test)\n",
        "  mod = MultinomialNB()\n",
        "  mod.fit(tfidfv, y_train)\n",
        "  predicted = mod.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
        "  print(\"Multinomial NB 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
        "\n",
        "def ComplementNaiveBayesClasifier(x_train, y_train, x_test, y_test):\n",
        "  tfidfv, tfidfv_test = makeDtmTfIdf(x_train, x_test)\n",
        "  cb = ComplementNB()\n",
        "  cb.fit(tfidfv, y_train)\n",
        "  predicted = cb.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
        "  print(\"Complement Naive Bayes 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
        "\n",
        "\n",
        "def LogisticRegressionClassifier(x_train, y_train, x_test, y_test):\n",
        "  tfidfv, tfidfv_test = makeDtmTfIdf(x_train, x_test)\n",
        "\n",
        "  lr = LogisticRegression(C=10000, penalty='l2')\n",
        "  lr.fit(tfidfv, y_train)\n",
        "  predicted = lr.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
        "  print(\"Logistic Regression 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
        "\n",
        "def LinearSVM(x_train, y_train, x_test, y_test):\n",
        "  tfidfv, tfidfv_test = makeDtmTfIdf(x_train, x_test)\n",
        "\n",
        "  lsvc = LinearSVC(C=1000, penalty='l1', max_iter=500, dual=False)\n",
        "  lsvc.fit(tfidfv, y_train)\n",
        "\n",
        "  predicted = lsvc.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
        "  print(\"LinearSVM 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
        "\n",
        "def DTClassifier(x_train, y_train, x_test, y_test):\n",
        "  tree = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
        "  tfidfv, tfidfv_test = makeDtmTfIdf(x_train, x_test)\n",
        "  tree.fit(tfidfv, y_train)\n",
        "  predicted = tree.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
        "  print(\"Decesion Tree Classifier 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
        "\n",
        "def RFClassifier(x_train, y_train, x_test, y_test):\n",
        "  tfidfv, tfidfv_test = makeDtmTfIdf(x_train, x_test)\n",
        "  forest = RandomForestClassifier(n_estimators=5, random_state=0)\n",
        "  forest.fit(tfidfv, y_train)\n",
        "  predicted = forest.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
        "  print(\"RandomForest 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
        "\n",
        "def GBClassifier(x_train, y_train, x_test, y_test):\n",
        "  tfidfv, tfidfv_test = makeDtmTfIdf(x_train, x_test)\n",
        "  grbt = GradientBoostingClassifier(random_state=0) # verbose=3\n",
        "  grbt.fit(tfidfv, y_train)\n",
        "  predicted = grbt.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
        "  print(\"Gradient Boosting 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
        "\n",
        "def VTClassifier(x_train, y_train, x_test, y_test):\n",
        "  tfidfv, tfidfv_test = makeDtmTfIdf(x_train, x_test)\n",
        "  voting_classifier = VotingClassifier(estimators=[\n",
        "         ('lr', LogisticRegression(C=10000, penalty='l2')),\n",
        "        ('cb', ComplementNB()),\n",
        "        ('grbt', GradientBoostingClassifier(random_state=0))\n",
        "        ], voting='soft', n_jobs=-1)\n",
        "  voting_classifier.fit(tfidfv, y_train)\n",
        "\n",
        "  predicted = voting_classifier.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
        "  print(\"Voting Classifier 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
        "\n",
        "def RunAllClasifier(x_train, y_train, x_test, y_test):\n",
        "  \n",
        "  cf_list= [MultinomialNaiveBayesClassifier, ComplementNaiveBayesClasifier, \n",
        "            LogisticRegressionClassifier, LinearSVM, DTClassifier, RFClassifier, \n",
        "            GBClassifier]\n",
        "  for f in cf_list:\n",
        "    f(x_train, y_train, x_test, y_test)\n",
        "\n",
        "\n",
        "\n",
        "def AddSpecialIndex(index_to_word, data):\n",
        "\n",
        "  decoded = []\n",
        "  for i in range(len(data)):\n",
        "    t = ' '.join([index_to_word[index] for index in data[i]])\n",
        "    decoded.append(t)\n",
        "\n",
        "  return decoded\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80T_VPBPZDI9"
      },
      "source": [
        "# 1. 모든 단어 사용\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R10oOrFxYw6d",
        "outputId": "2c69c26e-e79d-49a1-a08e-4121aec8b156"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)\n",
        "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
        "index_to_word = {index + 3 : word for word, index in word_index.items()}\n",
        "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
        "    index_to_word[index]=token\n",
        "\n",
        "x_train = AddSpecialIndex(index_to_word, x_train)\n",
        "x_test = AddSpecialIndex(index_to_word, x_test)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters_word_index.json\n",
            "557056/550378 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYFpe3NLgcc4",
        "outputId": "5ab896d1-9a56-4991-b00d-8b7313f32fae"
      },
      "source": [
        "RunAllClasifier(x_train, y_train, x_test, y_test)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Multinomial NB 정확도: 0.5997328584149599\n",
            "Complement Naive Bayes 정확도: 0.7649154051647373\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression 정확도: 0.813446126447017\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LinearSVM 정확도: 0.7880676758682101\n",
            "Decesion Tree Classifier 정확도: 0.6211041852181657\n",
            "RandomForest 정확도: 0.6544968833481746\n",
            "Gradient Boosting 정확도: 0.7684772929652716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VPqWq7rrqBr"
      },
      "source": [
        "# 빈도수 상위 5000개의 단어만 사용\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bZNK-zNgwiO",
        "outputId": "4265a02c-6095-43cd-b60d-84a56fcd47d4"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=5000, test_split=0.2)\n",
        "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
        "index_to_word = {index + 3 : word for word, index in word_index.items()}\n",
        "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
        "    index_to_word[index]=token\n",
        "\n",
        "x_train = AddSpecialIndex(index_to_word, x_train)\n",
        "x_test = AddSpecialIndex(index_to_word, x_test)\n",
        "RunAllClasifier(x_train, y_train, x_test, y_test)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Multinomial NB 정확도: 0.6731967943009796\n",
            "Complement Naive Bayes 정확도: 0.7707034728406055\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression 정확도: 0.8058771148708815\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LinearSVM 정확도: 0.7666963490650045\n",
            "Decesion Tree Classifier 정확도: 0.6179875333926982\n",
            "RandomForest 정확도: 0.701246660730187\n",
            "Gradient Boosting 정확도: 0.769813000890472\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVy3dSY60jJU",
        "outputId": "da64dc7f-1c40-4393-8b20-1b759d4f976e"
      },
      "source": [
        "x_train[:10]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<sos> <unk> <unk> said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3',\n",
              " '<sos> generale de banque sa lt <unk> <unk> and lt heller overseas corp of chicago have each taken 50 pct stakes in <unk> company sa <unk> factors generale de banque said in a statement it gave no financial details of the transaction sa <unk> <unk> turnover in 1986 was 17 5 billion belgian francs reuter 3',\n",
              " '<sos> shr 3 28 dlrs vs 22 cts shr diluted 2 99 dlrs vs 22 cts net 46 0 mln vs 3 328 000 avg shrs 14 0 mln vs 15 2 mln year shr 5 41 dlrs vs 1 56 dlrs shr diluted 4 94 dlrs vs 1 50 dlrs net 78 2 mln vs 25 9 mln avg shrs 14 5 mln vs 15 1 mln note earnings per share reflect the two for one split effective january 6 1987 per share amounts are calculated after preferred stock dividends loss continuing operations for the qtr 1986 includes gains of sale of investments in <unk> corp of 14 mln dlrs and associated companies of 4 189 000 less writedowns of investments in national <unk> inc of 11 8 mln and <unk> corp of 15 6 mln reuter 3',\n",
              " \"<sos> the farmers home administration the u s agriculture department's farm lending arm could lose about seven billion dlrs in outstanding principal on its severely <unk> borrowers or about one fourth of its farm loan portfolio the general accounting office gao said in remarks prepared for delivery to the senate agriculture committee brian <unk> senior associate director of gao also said that a preliminary analysis of proposed changes in <unk> financial <unk> standards indicated as many as one half of <unk> borrowers who received new loans from the agency in 1986 would be <unk> under the proposed system the agency has proposed <unk> <unk> credit using a variety of financial <unk> instead of <unk> <unk> on <unk> ability senate agriculture committee chairman <unk> <unk> d <unk> <unk> the proposed <unk> changes telling <unk> administrator <unk> clark at a hearing that they would mark a dramatic shift in the <unk> purpose away from being <unk> <unk> of last <unk> toward becoming a big city bank but clark <unk> the new regulations saying the agency had a responsibility to <unk> its 70 billion dlr loan portfolio in a <unk> yet <unk> manner <unk> of gao <unk> <unk> arm said the proposed credit <unk> system attempted to ensure that <unk> would make loans only to borrowers who had a reasonable change of <unk> their debt reuter 3\",\n",
              " '<sos> <unk> co said its board has received a proposal from chairman and chief executive officer philip d <unk> to acquire <unk> for 15 75 dlrs per share in cash <unk> said the acquisition bid is subject to <unk> arranging the necessary financing it said he intends to ask other members of senior management to participate the company said <unk> owns 30 pct of <unk> stock and other management members another 7 5 pct <unk> said it has formed an independent board committee to consider the offer and has deferred the annual meeting it had scheduled for march 31 reuter 3',\n",
              " \"<sos> the u s agriculture department estimated canada's 1986 87 wheat crop at 31 85 mln tonnes vs 31 85 mln tonnes last month it estimated 1985 86 output at 24 25 mln tonnes vs 24 25 mln last month canadian 1986 87 coarse grain production is projected at 27 62 mln tonnes vs 27 62 mln tonnes last month production in 1985 86 is estimated at 24 95 mln tonnes vs 24 95 mln last month canadian wheat exports in 1986 87 are forecast at 19 00 mln tonnes vs 18 00 mln tonnes last month exports in 1985 86 are estimated at 17 71 mln tonnes vs 17 72 mln last month reuter 3\",\n",
              " '<sos> lt <unk> america ltd said it is again extending its offer of 13 dlrs a share for 3 3 mln <unk> development corp shares until today from yesterday at midnight yesterday 7 242 117 <unk> shares had been tendered up from 5 <unk> 165 shares 24 hours earlier <unk> said it is extending the offer to allow <unk> to comply with federal law <unk> the ownership of u s airlines by non u s citizens and to <unk> the terms and conditions of the letter of credit or bank guarantee required under the previously announced acquisition agreement reuter 3',\n",
              " '<sos> shr 49 cts vs 39 cts net <unk> <unk> vs 892 323 revs 25 9 mln vs 23 7 mln year shr 1 78 dlr vs 1 34 dlr net 3 254 301 vs 2 472 <unk> revs 100 6 mln vs 87 4 mln note 1986 4th qtr and year net includes income loss of <unk> subsidiary of 14 <unk> dlrs and 311 <unk> dlrs or 17 cts per share respectively 1985 4th qtr and year net includes loss in <unk> unit of 108 598 dlrs and 298 <unk> dlrs or 16 cts per share respectively reuter 3',\n",
              " '<sos> oper shr 23 cts vs 77 cts oper net 5 255 179 vs 17 6 mln revs 37 8 mln vs 73 7 mln note cash flow 19 5 mln dlrs or 86 cts shr vs 36 7 mln dlrs or 1 62 dlrs shr 1985 net excludes 32 ct shr loss from discontinued operations gross proven and probable reserves of crude oil and natural gas <unk> 18 4 mln barrels off 7 6 pct from a year before reuter 3',\n",
              " '<sos> lt <unk> corp has accepted japanese <unk> to lift a higher proportion of arab heavy crude oil under term contracts in july oil industry sources said japanese companies requested a ratio of 80 pct arab heavy to 20 pct arab light under a term contract agreement with <unk> for 100 000 barrels per day the sources said the <unk> ratio is 30 pct heavy crude to 70 pct light japanese demand for heavy crude oil has increased substantially since the all japan <unk> union <unk> <unk> into the northern mideast gulf last month causing problems with <unk> of heavy kuwait and <unk> crudes reuter 3']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19dtSj3_s71C"
      },
      "source": [
        "# 딥러닝 모델 적용하기 (LSTM 사용)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1H6zqT1smDl"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "vocab_size = 5000\n",
        "\n",
        "def train_LSTM(X_train, y_train):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, 100))\n",
        "    model.add(LSTM(128))\n",
        "    model.add(Dense(46, activation='sigmoid')) # 46개의 class를 분류해야한다.\n",
        "\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "    mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "    model.compile(optimizer='rmsprop', loss='CategoricalCrossentropy', metrics=['acc'])\n",
        "    history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)\n",
        "\n",
        "    return model, history"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGO2yvrA0Mmz",
        "outputId": "a8a06ed0-8348-4bac-e800-238b9e577238"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=vocab_size, test_split=0.2)\n",
        "index_to_word = {index + 3 : word for word, index in word_index.items()}\n",
        "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
        "    index_to_word[index]=token\n",
        "\n",
        "x_train = AddSpecialIndex(index_to_word, x_train)\n",
        "x_test = AddSpecialIndex(index_to_word, x_test)\n",
        "\n",
        "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV')\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "X_train = tokenizer.texts_to_sequences(x_train)\n",
        "X_test = tokenizer.texts_to_sequences(x_test)\n",
        "max_len = 50\n",
        "X_train = pad_sequences(X_train, maxlen = max_len)\n",
        "X_test = pad_sequences(X_test, maxlen = max_len)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRqajNDl0b43",
        "outputId": "90639317-6485-4a8a-8a7f-651af62b6585"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "\n",
        "model, history = train_LSTM(X_train, y_train)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "120/120 [==============================] - 32s 15ms/step - loss: 2.5918 - acc: 0.3531 - val_loss: 1.9613 - val_acc: 0.4407\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.44073, saving model to best_model.h5\n",
            "Epoch 2/15\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 1.8868 - acc: 0.4818 - val_loss: 1.7109 - val_acc: 0.5392\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.44073 to 0.53923, saving model to best_model.h5\n",
            "Epoch 3/15\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 1.6590 - acc: 0.5478 - val_loss: 1.6140 - val_acc: 0.5748\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.53923 to 0.57485, saving model to best_model.h5\n",
            "Epoch 4/15\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 1.4571 - acc: 0.6044 - val_loss: 1.5378 - val_acc: 0.5882\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.57485 to 0.58820, saving model to best_model.h5\n",
            "Epoch 5/15\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 1.2711 - acc: 0.6560 - val_loss: 1.4551 - val_acc: 0.6322\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.58820 to 0.63216, saving model to best_model.h5\n",
            "Epoch 6/15\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 1.1240 - acc: 0.6984 - val_loss: 1.4271 - val_acc: 0.6366\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.63216 to 0.63662, saving model to best_model.h5\n",
            "Epoch 7/15\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 1.0202 - acc: 0.7202 - val_loss: 1.3865 - val_acc: 0.6533\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.63662 to 0.65331, saving model to best_model.h5\n",
            "Epoch 8/15\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.9354 - acc: 0.7466 - val_loss: 1.4771 - val_acc: 0.6183\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.65331\n",
            "Epoch 9/15\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.8544 - acc: 0.7668 - val_loss: 1.4745 - val_acc: 0.6566\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.65331 to 0.65665, saving model to best_model.h5\n",
            "Epoch 10/15\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.7819 - acc: 0.7937 - val_loss: 1.4374 - val_acc: 0.6678\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.65665 to 0.66778, saving model to best_model.h5\n",
            "Epoch 11/15\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.7189 - acc: 0.8025 - val_loss: 1.4472 - val_acc: 0.6661\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.66778\n",
            "Epoch 00011: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4_p4OUh2P5F",
        "outputId": "6c9a08a3-5dbd-4870-d295-ac2c3b1f46ad"
      },
      "source": [
        "y_test = to_categorical(y_test)\n",
        "\n",
        "results = model.evaluate(X_test,  y_test, verbose=2)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "71/71 - 0s - loss: 1.5660 - acc: 0.6385\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tchpr62B85ku"
      },
      "source": [
        "# 회고\n",
        "LSTM으로 돌려보면 기존 머신러닝 방법보다 성능이 못하다.\n",
        "이유는 tokenization과정을 공백으로만 했고, 영어의 경우 a, the와 같은 불용어를 빼주지 않은 이유도 있을거 같다.\n",
        "그리고 단어를 단순히 숫자로만 변경했다. w2v를 이용하면 성능이 좋아질거 같은데 귀찮..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw_Qs2xQ82PQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}